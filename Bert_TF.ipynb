{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.parsing.preprocessing import strip_short, remove_stopwords, strip_non_alphanum, strip_numeric, strip_multiple_whitespaces, strip_punctuation, strip_tags\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "#import requests\n",
    "#request = requests.get(\"https://drive.google.com/uc?export=download&id=1wHt8PsMLsfX5yNSqrt2fSTcb8LEiclcf\")\n",
    "#with open(\"data.zip\", \"wb\") as file:\n",
    "#    file.write(request.content)\n",
    "\n",
    "# Unzip data\n",
    "#import zipfile\n",
    "#with zipfile.ZipFile('data.zip') as zip:\n",
    "#    zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load data and set labels\n",
    "#data_complaint = pd.read_csv('data/complaint1700.csv')\n",
    "#data_complaint['label'] = 0\n",
    "#data_non_complaint = pd.read_csv('data/noncomplaint1700.csv')\n",
    "#data_non_complaint['label'] = 1\n",
    "\n",
    "# Concatenate complaining and non-complaining data\n",
    "#data = pd.concat([data_complaint, data_non_complaint], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Drop 'airline' column\n",
    "#data.drop(['airline'], inplace=True, axis=1)\n",
    "\n",
    "# Display 5 random samples\n",
    "#data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD TWEET DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv('data/tweets.csv')\n",
    "data = pd.read_csv('data/auto_tweets.csv')\n",
    "\n",
    "data = data.sample(frac = 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['auto_sentiment'] = data['auto_sentiment'].map(lambda x: x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data[:10000]\n",
    "data = data[10000:]\n",
    "\n",
    "test_data = test_data[:10000]\n",
    "data = data[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>auto_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1440517</th>\n",
       "      <td>20 min. after twelve...just finished making a ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1235950</th>\n",
       "      <td>......never drive faSter than yOur guardian an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035855</th>\n",
       "      <td>@RowdyKittens  Well, a tiny piece of the world...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211992</th>\n",
       "      <td>Had a long day n now I'm trying 2 chill with t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896863</th>\n",
       "      <td>@angelicaeff WELL HEY HEY BABY ITS NEVER TOO L...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550683</th>\n",
       "      <td>Road trip ! To birmingham  wat a shit whole !</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132065</th>\n",
       "      <td>My sister got the kids &amp;amp; Up. I got my dad ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578880</th>\n",
       "      <td>@kestrelwines yay! so glad to see u on twitter...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517324</th>\n",
       "      <td>@Live_for_Films Oh, bum.  I love that book. Ho...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208642</th>\n",
       "      <td>My day officially sucks now! Sure wish I could...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment  \\\n",
       "1440517  20 min. after twelve...just finished making a ...          1   \n",
       "1235950  ......never drive faSter than yOur guardian an...          1   \n",
       "1035855  @RowdyKittens  Well, a tiny piece of the world...          1   \n",
       "1211992  Had a long day n now I'm trying 2 chill with t...          1   \n",
       "896863   @angelicaeff WELL HEY HEY BABY ITS NEVER TOO L...          1   \n",
       "550683       Road trip ! To birmingham  wat a shit whole !          0   \n",
       "1132065  My sister got the kids &amp; Up. I got my dad ...          1   \n",
       "1578880  @kestrelwines yay! so glad to see u on twitter...          1   \n",
       "517324   @Live_for_Films Oh, bum.  I love that book. Ho...          0   \n",
       "208642   My day officially sucks now! Sure wish I could...          0   \n",
       "\n",
       "         auto_sentiment  \n",
       "1440517               1  \n",
       "1235950               1  \n",
       "1035855               2  \n",
       "1211992               1  \n",
       "896863                0  \n",
       "550683                0  \n",
       "1132065               1  \n",
       "1578880               2  \n",
       "517324                2  \n",
       "208642                2  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data= data[:1000]\n",
    "data.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.text.values\n",
    "y = data.auto_sentiment.values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>auto_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>817319</th>\n",
       "      <td>Pursuing a new business venture. Nervous and e...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089648</th>\n",
       "      <td>@Lumiel Aren't you sweet?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978914</th>\n",
       "      <td>@blissmanu I started at 15:50 and reached the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212878</th>\n",
       "      <td>@alexandramusic Diversity won! damnn i wanted ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099452</th>\n",
       "      <td>@nofe do you really want to spend 4 months doi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment  \\\n",
       "817319   Pursuing a new business venture. Nervous and e...          1   \n",
       "1089648                         @Lumiel Aren't you sweet?           1   \n",
       "978914   @blissmanu I started at 15:50 and reached the ...          1   \n",
       "212878   @alexandramusic Diversity won! damnn i wanted ...          0   \n",
       "1099452  @nofe do you really want to spend 4 months doi...          1   \n",
       "\n",
       "         auto_sentiment  \n",
       "817319                2  \n",
       "1089648               0  \n",
       "978914                2  \n",
       "212878                2  \n",
       "1099452               2  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test data\n",
    "#test_data = pd.read_csv('data/test_data.csv')\n",
    "\n",
    "# Keep important columns\n",
    "#test_data = test_data[['id', 'tweet']]\n",
    "\n",
    "# Display 5 samples from the test data\n",
    "test_data.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/felix/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# Uncomment to download \"stopwords\"\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#def text_preprocessing(s):\n",
    "#    \"\"\"\n",
    "#    - Lowercase the sentence\n",
    "#    - Change \"'t\" to \"not\"\n",
    "#    - Remove \"@name\"\n",
    "#    - Isolate and remove punctuations except \"?\"\n",
    "#    - Remove other special characters\n",
    "#    - Remove stop words except \"not\" and \"can\"\n",
    "#    - Remove trailing whitespace\n",
    "#    \"\"\"\n",
    "#    s = s.lower()\n",
    "#    # Change 't to 'not'\n",
    "#    s = re.sub(r\"\\'t\", \" not\", s)\n",
    "#    # Remove @name\n",
    "#    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n",
    "#    # Isolate and remove punctuations except '?'\n",
    "#    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\/\\,])', r' \\1 ', s)\n",
    "#    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n",
    "#    # Remove some special characters\n",
    "#    s = re.sub(r'([\\;\\:\\|•«\\n])', ' ', s)\n",
    "#    # Remove stopwords except 'not' and 'can'\n",
    "#    s = \" \".join([word for word in s.split()\n",
    "#                  if word not in stopwords.words('english')\n",
    "#                  or word in ['not', 'can']])\n",
    "#    # Remove trailing whitespace\n",
    "#    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    \n",
    "#    return s\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    text = strip_tags(text)\n",
    "    text = strip_non_alphanum(text)\n",
    "    text = strip_punctuation(text)\n",
    "    text = strip_numeric(text)\n",
    "    #text = remove_stopwords(text)\n",
    "    text = text.lower()\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    #text = strip_short(text, minsize = 3)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.74 s, sys: 28 ms, total: 1.76 s\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Preprocess text\n",
    "X_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\n",
    "X_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n",
    "\n",
    "# Calculate TF-IDF\n",
    "#tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "#                         binary=True,\n",
    "#                         smooth_idf=False)\n",
    "#X_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\n",
    "#X_val_tfidf = tf_idf.transform(X_val_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['got my tickets for the Blink 182 irvine show  be jealous.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[38:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'got my tickets for the blink irvine show be jealous '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_preprocessed[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing_for_bert([X_train_preprocessed[38]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "def get_auc_CV(model):\n",
    "    \"\"\"\n",
    "    Return the average AUC score from cross-validation.\n",
    "    \"\"\"\n",
    "    # Set KFold to shuffle data before the split\n",
    "    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n",
    "\n",
    "    # Get AUC scores\n",
    "    auc = cross_val_score(\n",
    "        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n",
    "\n",
    "    return auc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#res = pd.Series([get_auc_CV(MultinomialNB(i))\n",
    "#                 for i in np.arange(1, 10, 0.1)],\n",
    "#                index=np.arange(1, 10, 0.1))\n",
    "\n",
    "#best_alpha = np.round(res.idxmax(), 2)\n",
    "#print('Best alpha: ', best_alpha)\n",
    "\n",
    "#plt.plot(res)\n",
    "#plt.title('AUC vs. Alpha')\n",
    "#plt.xlabel('Alpha')\n",
    "#plt.ylabel('AUC')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities\n",
    "#nb_model = MultinomialNB(alpha=1.8)\n",
    "#nb_model.fit(X_train_tfidf, y_train)\n",
    "#probs = nb_model.predict_proba(X_val_tfidf)\n",
    "\n",
    "# Evaluate the classifier\n",
    "#evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def text_preprocessing(text):\n",
    "#    \"\"\"\n",
    "#    - Remove entity mentions (eg. '@united')\n",
    "#    - Correct errors (eg. '&amp;' to '&')\n",
    "#    @param    text (str): a string to be processed.\n",
    "#    @return   text (Str): the processed string.\n",
    "#    \"\"\"\n",
    "#    # Remove '@name'\n",
    "#    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "#\n",
    "#    # Replace '&amp;' with '&'\n",
    "#    text = re.sub(r'&amp;', '&', text)\n",
    "#\n",
    "#    # Remove trailing whitespace\n",
    "#    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#    return text\n",
    "\n",
    "#def text_preprocessing(text):\n",
    "#    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "#    text = strip_tags(text)\n",
    "#    text = strip_non_alphanum(text)\n",
    "#    text = strip_punctuation(text)\n",
    "#    text = strip_numeric(text)\n",
    "#    #text = remove_stopwords(text)\n",
    "#    text = text.lower()\n",
    "#    text = strip_multiple_whitespaces(text)\n",
    "#    #text = strip_short(text, minsize = 3)\n",
    "#\n",
    "#    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180565    No Windows 7 support for VMware View yet.  (VM...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        #print(\"aaaa: \", tokenizer.convert_ids_to_tokens(encoded_sent.get('input_ids')))\n",
    "        #print(\"sent: \" , sent)\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  216\n"
     ]
    }
   ],
   "source": [
    "# Concatenate train data and test data\n",
    "#all_tweets = np.concatenate([data.tweet.values, test_data.tweet.values])\n",
    "all_tweets = np.concatenate([data.text.values, test_data.text.values])\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_tweets]) #230\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  No Windows 7 support for VMware View yet.  (VMwareInfo.com) http://bit.ly/YIsYA\n",
      "Token IDs:  [101, 2053, 3645, 2490, 2005, 1058, 2213, 8059, 3193, 2664, 1058, 2213, 8059, 2378, 14876, 4012, 8299, 2978, 1048, 2100, 12316, 6508, 2050, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = max_len\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 µs, sys: 0 ns, total: 33 µs\n",
      "Wall time: 36.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 50, 3\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits, dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   1.018215   |     -      |     -     |   12.96  \n",
      "   1    |   40    |   0.902504   |     -      |     -     |   11.61  \n",
      "   1    |   60    |   0.717939   |     -      |     -     |   11.32  \n",
      "   1    |   80    |   0.645551   |     -      |     -     |   11.40  \n",
      "   1    |   100   |   0.583537   |     -      |     -     |   11.38  \n",
      "   1    |   120   |   0.553276   |     -      |     -     |   11.39  \n",
      "   1    |   140   |   0.520482   |     -      |     -     |   11.39  \n",
      "   1    |   160   |   0.516909   |     -      |     -     |   11.57  \n",
      "   1    |   180   |   0.482380   |     -      |     -     |   11.73  \n",
      "   1    |   200   |   0.444277   |     -      |     -     |   11.48  \n",
      "   1    |   220   |   0.464593   |     -      |     -     |   11.43  \n",
      "   1    |   240   |   0.497613   |     -      |     -     |   11.52  \n",
      "   1    |   260   |   0.407713   |     -      |     -     |   11.45  \n",
      "   1    |   280   |   0.417269   |     -      |     -     |   11.47  \n",
      "   1    |   300   |   0.437199   |     -      |     -     |   11.86  \n",
      "   1    |   320   |   0.391812   |     -      |     -     |   11.66  \n",
      "   1    |   340   |   0.439352   |     -      |     -     |   11.46  \n",
      "   1    |   360   |   0.373972   |     -      |     -     |   11.56  \n",
      "   1    |   380   |   0.414542   |     -      |     -     |   11.43  \n",
      "   1    |   400   |   0.402402   |     -      |     -     |   11.70  \n",
      "   1    |   420   |   0.340366   |     -      |     -     |   11.76  \n",
      "   1    |   440   |   0.366372   |     -      |     -     |   11.61  \n",
      "   1    |   460   |   0.353254   |     -      |     -     |   11.45  \n",
      "   1    |   480   |   0.320364   |     -      |     -     |   11.46  \n",
      "   1    |   500   |   0.433614   |     -      |     -     |   11.74  \n",
      "   1    |   520   |   0.313986   |     -      |     -     |   11.70  \n",
      "   1    |   540   |   0.421623   |     -      |     -     |   11.45  \n",
      "   1    |   560   |   0.387843   |     -      |     -     |   11.47  \n",
      "   1    |   580   |   0.395613   |     -      |     -     |   11.46  \n",
      "   1    |   600   |   0.327926   |     -      |     -     |   11.47  \n",
      "   1    |   620   |   0.345073   |     -      |     -     |   11.47  \n",
      "   1    |   640   |   0.368678   |     -      |     -     |   11.43  \n",
      "   1    |   660   |   0.335065   |     -      |     -     |   11.43  \n",
      "   1    |   680   |   0.305033   |     -      |     -     |   11.45  \n",
      "   1    |   700   |   0.411304   |     -      |     -     |   11.47  \n",
      "   1    |   720   |   0.288603   |     -      |     -     |   11.70  \n",
      "   1    |   740   |   0.392488   |     -      |     -     |   11.47  \n",
      "   1    |   760   |   0.285408   |     -      |     -     |   11.74  \n",
      "   1    |   780   |   0.268421   |     -      |     -     |   11.71  \n",
      "   1    |   800   |   0.351252   |     -      |     -     |   11.55  \n",
      "   1    |   820   |   0.353289   |     -      |     -     |   11.41  \n",
      "   1    |   840   |   0.345100   |     -      |     -     |   11.43  \n",
      "   1    |   860   |   0.305282   |     -      |     -     |   11.47  \n",
      "   1    |   880   |   0.302889   |     -      |     -     |   11.42  \n",
      "   1    |   900   |   0.303392   |     -      |     -     |   11.40  \n",
      "   1    |   920   |   0.325824   |     -      |     -     |   11.40  \n",
      "   1    |   940   |   0.344903   |     -      |     -     |   11.41  \n",
      "   1    |   960   |   0.324351   |     -      |     -     |   11.41  \n",
      "   1    |   980   |   0.272722   |     -      |     -     |   11.41  \n",
      "   1    |  1000   |   0.355583   |     -      |     -     |   11.41  \n",
      "   1    |  1020   |   0.323276   |     -      |     -     |   11.40  \n",
      "   1    |  1040   |   0.260736   |     -      |     -     |   11.40  \n",
      "   1    |  1060   |   0.353508   |     -      |     -     |   11.49  \n",
      "   1    |  1080   |   0.300921   |     -      |     -     |   11.56  \n",
      "   1    |  1100   |   0.284382   |     -      |     -     |   11.47  \n",
      "   1    |  1120   |   0.303601   |     -      |     -     |   11.56  \n",
      "   1    |  1140   |   0.296907   |     -      |     -     |   11.55  \n",
      "   1    |  1160   |   0.248675   |     -      |     -     |   11.62  \n",
      "   1    |  1180   |   0.210184   |     -      |     -     |   11.63  \n",
      "   1    |  1200   |   0.302860   |     -      |     -     |   11.56  \n",
      "   1    |  1220   |   0.261089   |     -      |     -     |   11.49  \n",
      "   1    |  1240   |   0.245411   |     -      |     -     |   11.63  \n",
      "   1    |  1260   |   0.231681   |     -      |     -     |   11.40  \n",
      "   1    |  1280   |   0.271778   |     -      |     -     |   11.39  \n",
      "   1    |  1300   |   0.238191   |     -      |     -     |   11.39  \n",
      "   1    |  1320   |   0.251886   |     -      |     -     |   11.43  \n",
      "   1    |  1340   |   0.195941   |     -      |     -     |   11.60  \n",
      "   1    |  1360   |   0.314259   |     -      |     -     |   11.77  \n",
      "   1    |  1380   |   0.253058   |     -      |     -     |   11.75  \n",
      "   1    |  1400   |   0.233378   |     -      |     -     |   11.64  \n",
      "   1    |  1420   |   0.331142   |     -      |     -     |   11.44  \n",
      "   1    |  1440   |   0.319976   |     -      |     -     |   11.40  \n",
      "   1    |  1460   |   0.220079   |     -      |     -     |   11.43  \n",
      "   1    |  1480   |   0.328537   |     -      |     -     |   11.38  \n",
      "   1    |  1500   |   0.236528   |     -      |     -     |   11.40  \n",
      "   1    |  1520   |   0.264168   |     -      |     -     |   11.52  \n",
      "   1    |  1540   |   0.253347   |     -      |     -     |   11.63  \n",
      "   1    |  1560   |   0.271559   |     -      |     -     |   11.46  \n",
      "   1    |  1580   |   0.291639   |     -      |     -     |   11.37  \n",
      "   1    |  1600   |   0.262100   |     -      |     -     |   11.38  \n",
      "   1    |  1620   |   0.260705   |     -      |     -     |   11.41  \n",
      "   1    |  1640   |   0.309196   |     -      |     -     |   11.46  \n",
      "   1    |  1660   |   0.271256   |     -      |     -     |   11.45  \n",
      "   1    |  1680   |   0.224572   |     -      |     -     |   11.47  \n",
      "   1    |  1700   |   0.265599   |     -      |     -     |   11.43  \n",
      "   1    |  1720   |   0.221715   |     -      |     -     |   11.44  \n",
      "   1    |  1740   |   0.258395   |     -      |     -     |   11.39  \n",
      "   1    |  1760   |   0.334203   |     -      |     -     |   11.60  \n",
      "   1    |  1780   |   0.256939   |     -      |     -     |   11.65  \n",
      "   1    |  1800   |   0.257559   |     -      |     -     |   11.52  \n",
      "   1    |  1820   |   0.326033   |     -      |     -     |   11.39  \n",
      "   1    |  1840   |   0.271831   |     -      |     -     |   11.49  \n",
      "   1    |  1860   |   0.240880   |     -      |     -     |   11.52  \n",
      "   1    |  1880   |   0.214795   |     -      |     -     |   11.46  \n",
      "   1    |  1900   |   0.256490   |     -      |     -     |   11.58  \n",
      "   1    |  1920   |   0.287285   |     -      |     -     |   11.38  \n",
      "   1    |  1940   |   0.274844   |     -      |     -     |   11.47  \n",
      "   1    |  1960   |   0.244954   |     -      |     -     |   11.60  \n",
      "   1    |  1980   |   0.277057   |     -      |     -     |   11.80  \n",
      "   1    |  2000   |   0.253637   |     -      |     -     |   11.65  \n",
      "   1    |  2020   |   0.251406   |     -      |     -     |   11.50  \n",
      "   1    |  2040   |   0.253378   |     -      |     -     |   11.65  \n",
      "   1    |  2060   |   0.251211   |     -      |     -     |   11.43  \n",
      "   1    |  2080   |   0.237812   |     -      |     -     |   11.38  \n",
      "   1    |  2100   |   0.224526   |     -      |     -     |   11.38  \n",
      "   1    |  2120   |   0.270993   |     -      |     -     |   11.48  \n",
      "   1    |  2140   |   0.213276   |     -      |     -     |   11.68  \n",
      "   1    |  2160   |   0.256312   |     -      |     -     |   11.72  \n",
      "   1    |  2180   |   0.278301   |     -      |     -     |   11.57  \n",
      "   1    |  2200   |   0.262424   |     -      |     -     |   11.54  \n",
      "   1    |  2220   |   0.279154   |     -      |     -     |   11.41  \n",
      "   1    |  2240   |   0.253788   |     -      |     -     |   11.43  \n",
      "   1    |  2260   |   0.233601   |     -      |     -     |   11.67  \n",
      "   1    |  2280   |   0.247984   |     -      |     -     |   12.28  \n",
      "   1    |  2300   |   0.262315   |     -      |     -     |   12.13  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |  2320   |   0.256011   |     -      |     -     |   11.67  \n",
      "   1    |  2340   |   0.240992   |     -      |     -     |   11.63  \n",
      "   1    |  2360   |   0.246192   |     -      |     -     |   11.59  \n",
      "   1    |  2380   |   0.201653   |     -      |     -     |   11.58  \n",
      "   1    |  2400   |   0.239893   |     -      |     -     |   11.65  \n",
      "   1    |  2420   |   0.186047   |     -      |     -     |   11.96  \n",
      "   1    |  2440   |   0.216591   |     -      |     -     |   11.56  \n",
      "   1    |  2460   |   0.200852   |     -      |     -     |   11.51  \n",
      "   1    |  2480   |   0.262165   |     -      |     -     |   11.70  \n",
      "   1    |  2500   |   0.248213   |     -      |     -     |   11.52  \n",
      "   1    |  2520   |   0.241637   |     -      |     -     |   11.57  \n",
      "   1    |  2540   |   0.226213   |     -      |     -     |   11.59  \n",
      "   1    |  2560   |   0.244370   |     -      |     -     |   11.51  \n",
      "   1    |  2580   |   0.192568   |     -      |     -     |   11.56  \n",
      "   1    |  2600   |   0.264149   |     -      |     -     |   11.70  \n",
      "   1    |  2620   |   0.262910   |     -      |     -     |   11.93  \n",
      "   1    |  2640   |   0.225000   |     -      |     -     |   11.88  \n",
      "   1    |  2660   |   0.301069   |     -      |     -     |   11.82  \n",
      "   1    |  2680   |   0.274926   |     -      |     -     |   11.86  \n",
      "   1    |  2700   |   0.177103   |     -      |     -     |   11.72  \n",
      "   1    |  2720   |   0.222503   |     -      |     -     |   11.66  \n",
      "   1    |  2740   |   0.222011   |     -      |     -     |   11.79  \n",
      "   1    |  2760   |   0.254609   |     -      |     -     |   11.62  \n",
      "   1    |  2780   |   0.253865   |     -      |     -     |   12.02  \n",
      "   1    |  2800   |   0.276645   |     -      |     -     |   11.83  \n",
      "   1    |  2812   |   0.181460   |     -      |     -     |   6.87   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.314400   |  0.245352  |   92.48   |  1688.62 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   2    |   20    |   0.143710   |     -      |     -     |   12.80  \n",
      "   2    |   40    |   0.229164   |     -      |     -     |   11.99  \n",
      "   2    |   60    |   0.215231   |     -      |     -     |   11.73  \n",
      "   2    |   80    |   0.196186   |     -      |     -     |   11.67  \n",
      "   2    |   100   |   0.195170   |     -      |     -     |   11.65  \n",
      "   2    |   120   |   0.186324   |     -      |     -     |   11.72  \n",
      "   2    |   140   |   0.159272   |     -      |     -     |   11.68  \n",
      "   2    |   160   |   0.235321   |     -      |     -     |   12.03  \n",
      "   2    |   180   |   0.198781   |     -      |     -     |   12.28  \n",
      "   2    |   200   |   0.196867   |     -      |     -     |   12.22  \n",
      "   2    |   220   |   0.203605   |     -      |     -     |   12.08  \n",
      "   2    |   240   |   0.181972   |     -      |     -     |   12.00  \n",
      "   2    |   260   |   0.187643   |     -      |     -     |   11.59  \n",
      "   2    |   280   |   0.153501   |     -      |     -     |   11.67  \n",
      "   2    |   300   |   0.211495   |     -      |     -     |   12.08  \n",
      "   2    |   320   |   0.207489   |     -      |     -     |   11.54  \n",
      "   2    |   340   |   0.172084   |     -      |     -     |   12.28  \n",
      "   2    |   360   |   0.146223   |     -      |     -     |   12.25  \n",
      "   2    |   380   |   0.179121   |     -      |     -     |   12.26  \n",
      "   2    |   400   |   0.256475   |     -      |     -     |   12.23  \n",
      "   2    |   420   |   0.204142   |     -      |     -     |   11.73  \n",
      "   2    |   440   |   0.192870   |     -      |     -     |   11.59  \n",
      "   2    |   460   |   0.173442   |     -      |     -     |   11.54  \n",
      "   2    |   480   |   0.239244   |     -      |     -     |   11.46  \n",
      "   2    |   500   |   0.160066   |     -      |     -     |   11.47  \n",
      "   2    |   520   |   0.163244   |     -      |     -     |   11.90  \n",
      "   2    |   540   |   0.179594   |     -      |     -     |   11.56  \n",
      "   2    |   560   |   0.173512   |     -      |     -     |   11.88  \n",
      "   2    |   580   |   0.159289   |     -      |     -     |   11.42  \n",
      "   2    |   600   |   0.158592   |     -      |     -     |   11.47  \n",
      "   2    |   620   |   0.149083   |     -      |     -     |   11.43  \n",
      "   2    |   640   |   0.188130   |     -      |     -     |   11.83  \n",
      "   2    |   660   |   0.230463   |     -      |     -     |   11.66  \n",
      "   2    |   680   |   0.181786   |     -      |     -     |   11.53  \n",
      "   2    |   700   |   0.201748   |     -      |     -     |   11.49  \n",
      "   2    |   720   |   0.195191   |     -      |     -     |   11.46  \n",
      "   2    |   740   |   0.194076   |     -      |     -     |   11.49  \n",
      "   2    |   760   |   0.212131   |     -      |     -     |   11.58  \n",
      "   2    |   780   |   0.230488   |     -      |     -     |   11.74  \n",
      "   2    |   800   |   0.206530   |     -      |     -     |   11.51  \n",
      "   2    |   820   |   0.135103   |     -      |     -     |   11.48  \n",
      "   2    |   840   |   0.171129   |     -      |     -     |   11.50  \n",
      "   2    |   860   |   0.191059   |     -      |     -     |   11.51  \n",
      "   2    |   880   |   0.190231   |     -      |     -     |   11.61  \n",
      "   2    |   900   |   0.200724   |     -      |     -     |   11.48  \n",
      "   2    |   920   |   0.171155   |     -      |     -     |   11.70  \n",
      "   2    |   940   |   0.189480   |     -      |     -     |   11.69  \n",
      "   2    |   960   |   0.192969   |     -      |     -     |   11.56  \n",
      "   2    |   980   |   0.148234   |     -      |     -     |   11.62  \n",
      "   2    |  1000   |   0.157905   |     -      |     -     |   11.55  \n",
      "   2    |  1020   |   0.155673   |     -      |     -     |   11.67  \n",
      "   2    |  1040   |   0.178001   |     -      |     -     |   12.04  \n",
      "   2    |  1060   |   0.169159   |     -      |     -     |   11.75  \n",
      "   2    |  1080   |   0.140946   |     -      |     -     |   12.28  \n",
      "   2    |  1100   |   0.176203   |     -      |     -     |   11.86  \n",
      "   2    |  1120   |   0.181505   |     -      |     -     |   11.57  \n",
      "   2    |  1140   |   0.142447   |     -      |     -     |   12.00  \n",
      "   2    |  1160   |   0.185668   |     -      |     -     |   11.63  \n",
      "   2    |  1180   |   0.149444   |     -      |     -     |   12.16  \n",
      "   2    |  1200   |   0.113528   |     -      |     -     |   11.86  \n",
      "   2    |  1220   |   0.177606   |     -      |     -     |   11.85  \n",
      "   2    |  1240   |   0.130424   |     -      |     -     |   11.72  \n",
      "   2    |  1260   |   0.183440   |     -      |     -     |   11.48  \n",
      "   2    |  1280   |   0.197844   |     -      |     -     |   11.63  \n",
      "   2    |  1300   |   0.153787   |     -      |     -     |   11.74  \n",
      "   2    |  1320   |   0.178015   |     -      |     -     |   11.48  \n",
      "   2    |  1340   |   0.150550   |     -      |     -     |   11.46  \n",
      "   2    |  1360   |   0.194970   |     -      |     -     |   11.49  \n",
      "   2    |  1380   |   0.138858   |     -      |     -     |   11.45  \n",
      "   2    |  1400   |   0.143809   |     -      |     -     |   11.46  \n",
      "   2    |  1420   |   0.137119   |     -      |     -     |   11.44  \n",
      "   2    |  1440   |   0.216433   |     -      |     -     |   11.49  \n",
      "   2    |  1460   |   0.218076   |     -      |     -     |   11.47  \n",
      "   2    |  1480   |   0.142763   |     -      |     -     |   11.57  \n",
      "   2    |  1500   |   0.152643   |     -      |     -     |   11.48  \n",
      "   2    |  1520   |   0.163652   |     -      |     -     |   11.44  \n",
      "   2    |  1540   |   0.130435   |     -      |     -     |   11.43  \n",
      "   2    |  1560   |   0.135101   |     -      |     -     |   11.88  \n",
      "   2    |  1580   |   0.200452   |     -      |     -     |   11.46  \n",
      "   2    |  1600   |   0.161466   |     -      |     -     |   11.57  \n",
      "   2    |  1620   |   0.174801   |     -      |     -     |   11.42  \n",
      "   2    |  1640   |   0.147484   |     -      |     -     |   11.50  \n",
      "   2    |  1660   |   0.133259   |     -      |     -     |   11.52  \n",
      "   2    |  1680   |   0.167592   |     -      |     -     |   11.54  \n",
      "   2    |  1700   |   0.130491   |     -      |     -     |   11.54  \n",
      "   2    |  1720   |   0.188695   |     -      |     -     |   11.74  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2    |  1740   |   0.158154   |     -      |     -     |   11.87  \n",
      "   2    |  1760   |   0.143108   |     -      |     -     |   11.48  \n",
      "   2    |  1780   |   0.126684   |     -      |     -     |   11.48  \n",
      "   2    |  1800   |   0.177529   |     -      |     -     |   11.50  \n",
      "   2    |  1820   |   0.218080   |     -      |     -     |   11.66  \n",
      "   2    |  1840   |   0.122699   |     -      |     -     |   11.53  \n",
      "   2    |  1860   |   0.173750   |     -      |     -     |   11.49  \n",
      "   2    |  1880   |   0.189682   |     -      |     -     |   11.52  \n",
      "   2    |  1900   |   0.145839   |     -      |     -     |   11.59  \n",
      "   2    |  1920   |   0.163030   |     -      |     -     |   11.61  \n",
      "   2    |  1940   |   0.189116   |     -      |     -     |   11.62  \n",
      "   2    |  1960   |   0.124833   |     -      |     -     |   11.80  \n",
      "   2    |  1980   |   0.152838   |     -      |     -     |   12.02  \n",
      "   2    |  2000   |   0.099069   |     -      |     -     |   11.49  \n",
      "   2    |  2020   |   0.148001   |     -      |     -     |   11.47  \n",
      "   2    |  2040   |   0.199789   |     -      |     -     |   11.86  \n",
      "   2    |  2060   |   0.131770   |     -      |     -     |   11.92  \n",
      "   2    |  2080   |   0.188695   |     -      |     -     |   11.80  \n",
      "   2    |  2100   |   0.176506   |     -      |     -     |   12.84  \n",
      "   2    |  2120   |   0.150817   |     -      |     -     |   12.88  \n",
      "   2    |  2140   |   0.164161   |     -      |     -     |   12.84  \n",
      "   2    |  2160   |   0.175708   |     -      |     -     |   12.90  \n",
      "   2    |  2180   |   0.156248   |     -      |     -     |   12.76  \n",
      "   2    |  2200   |   0.232431   |     -      |     -     |   13.02  \n",
      "   2    |  2220   |   0.161876   |     -      |     -     |   12.77  \n",
      "   2    |  2240   |   0.130456   |     -      |     -     |   11.45  \n",
      "   2    |  2260   |   0.147164   |     -      |     -     |   11.42  \n",
      "   2    |  2280   |   0.131264   |     -      |     -     |   11.49  \n",
      "   2    |  2300   |   0.224506   |     -      |     -     |   11.53  \n",
      "   2    |  2320   |   0.163089   |     -      |     -     |   11.53  \n",
      "   2    |  2340   |   0.151758   |     -      |     -     |   11.59  \n",
      "   2    |  2360   |   0.178115   |     -      |     -     |   11.79  \n",
      "   2    |  2380   |   0.159311   |     -      |     -     |   11.68  \n",
      "   2    |  2400   |   0.154428   |     -      |     -     |   12.09  \n",
      "   2    |  2420   |   0.134758   |     -      |     -     |   11.76  \n",
      "   2    |  2440   |   0.140838   |     -      |     -     |   11.66  \n",
      "   2    |  2460   |   0.156554   |     -      |     -     |   11.65  \n",
      "   2    |  2480   |   0.157578   |     -      |     -     |   11.51  \n",
      "   2    |  2500   |   0.104840   |     -      |     -     |   11.45  \n",
      "   2    |  2520   |   0.119183   |     -      |     -     |   11.49  \n",
      "   2    |  2540   |   0.132788   |     -      |     -     |   12.56  \n",
      "   2    |  2560   |   0.202453   |     -      |     -     |   12.15  \n",
      "   2    |  2580   |   0.176711   |     -      |     -     |   11.44  \n",
      "   2    |  2600   |   0.150891   |     -      |     -     |   11.50  \n",
      "   2    |  2620   |   0.130468   |     -      |     -     |   11.92  \n",
      "   2    |  2640   |   0.206327   |     -      |     -     |   11.83  \n",
      "   2    |  2660   |   0.136473   |     -      |     -     |   12.04  \n",
      "   2    |  2680   |   0.162369   |     -      |     -     |   12.34  \n",
      "   2    |  2700   |   0.117650   |     -      |     -     |   11.50  \n",
      "   2    |  2720   |   0.139428   |     -      |     -     |   11.77  \n",
      "   2    |  2740   |   0.143504   |     -      |     -     |   11.54  \n",
      "   2    |  2760   |   0.113295   |     -      |     -     |   11.44  \n",
      "   2    |  2780   |   0.123649   |     -      |     -     |   11.44  \n",
      "   2    |  2800   |   0.178402   |     -      |     -     |   11.59  \n",
      "   2    |  2812   |   0.156452   |     -      |     -     |   6.66   \n",
      "----------------------------------------------------------------------\n",
      "   2    |    -    |   0.169067   |  0.204554  |   94.30   |  1712.16 \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bert_classifier.pickle', 'wb') as handle:\n",
    "    pickle.dump(bert_classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./bert_classifier.pickle', 'rb') as handle:\n",
    "        bert_classifier = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits)\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs_val = bert_predict(bert_classifier, val_dataloader)\n",
    "\n",
    "probs_train = bert_predict(bert_classifier, train_dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "#evaluate_roc(probs, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.36054444444444445\n",
      "val acc:  0.943\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i , p in enumerate(probs_train):\n",
    "    if np.argmax(p) == y_train[i]:\n",
    "        correct +=1       \n",
    "print(\"train acc: \" , correct / len(y_train))\n",
    "\n",
    "\n",
    "correct = 0\n",
    "for i , p in enumerate(probs_val):\n",
    "    if np.argmax(p) == y_val[i]:\n",
    "        correct +=1       \n",
    "print(\"val acc: \" , correct / len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.36054444444444445\n",
      "val acc:  0.943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9435735960209247, 0.943, 0.9429835050765621, None)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "correct = 0\n",
    "y_train_pred = []\n",
    "for i , p in enumerate(probs_train):\n",
    "    y_train_pred.append(np.argmax(p))\n",
    "    if np.argmax(p) == y_train[i]:\n",
    "        correct +=1       \n",
    "print(\"train acc: \" , correct / len(y_train))\n",
    "\n",
    "\n",
    "correct = 0\n",
    "y_val_pred = []\n",
    "\n",
    "for i , p in enumerate(probs_val):\n",
    "    y_val_pred.append(np.argmax(p))\n",
    "    if np.argmax(p) == y_val[i]:\n",
    "        correct +=1       \n",
    "print(\"val acc: \" , correct / len(y_val))\n",
    "\n",
    "pr = precision_recall_fscore_support(y_val, y_val_pred, average='weighted')\n",
    "pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the train set and the validation set\n",
    "#full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n",
    "#full_train_sampler = RandomSampler(full_train_data)\n",
    "#full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n",
    "\n",
    "# Train the Bert Classifier on the entire training data\n",
    "#set_seed(42)\n",
    "#bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "#train(bert_classifier, full_train_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run `preprocessing_for_bert` on the test set\n",
    "#print('Tokenizing data...')\n",
    "#test_inputs, test_masks = preprocessing_for_bert(test_data.text)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "#test_dataset = TensorDataset(test_inputs, test_masks)\n",
    "#test_sampler = SequentialSampler(test_dataset)\n",
    "#test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "#probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Get predictions from the probabilities\n",
    "#threshold = 0.9\n",
    "#preds = np.where(probs[:, 1] > threshold, 1, 0)\n",
    "\n",
    "# Number of tweets predicted non-negative\n",
    "#print(\"Number of tweets predicted non-negative: \", preds.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = test_data[preds==1]\n",
    "#list(output.sample(20).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
